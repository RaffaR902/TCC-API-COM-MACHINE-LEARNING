{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3254dfba",
   "metadata": {},
   "source": [
    "# Treinamento, teste e avaliação com os imoveis filtrados a venda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec69f8",
   "metadata": {},
   "source": [
    "## Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce2c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import locale\n",
    "\n",
    "# Ferramentas do Scikit-learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Modelos a serem comparados\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d934",
   "metadata": {},
   "source": [
    "## Configurações de Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807cdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o locale para formatar valores em Reais (BRL)\n",
    "try:\n",
    "    locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')\n",
    "    pd.options.display.float_format = 'R$ {:,.2f}'.format\n",
    "except:\n",
    "    print(\"Locale 'pt_BR.UTF-8' não encontrado. Usando locale padrão.\")\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Configurações do Seaborn para os gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa891bfa",
   "metadata": {},
   "source": [
    "## Carregar e Preparar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8484c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de venda carregados com sucesso!\n",
      "\n",
      "Visão inicial dos dados:\n",
      "          tipo                  bairro     cidade objetivo  area_util  \\\n",
      "0         casa          americanopolis  são paulo  Locação   R$ 40.00   \n",
      "1         casa         vila_gumercindo  são paulo  Locação   R$ 25.00   \n",
      "2         casa         vila_gumercindo  são paulo  Locação   R$ 25.00   \n",
      "3         casa            vila_fachini  são paulo  Locação   R$ 50.00   \n",
      "4  apartamento  jardim_miriam_zona_sul  são paulo  Locação   R$ 30.00   \n",
      "\n",
      "   quartos  suites  vagas     preco  \n",
      "0        1       0      0 R$ 800.00  \n",
      "1        1       0      0 R$ 850.00  \n",
      "2        1       0      0 R$ 850.00  \n",
      "3        1       0      0 R$ 850.00  \n",
      "4        1       0      0 R$ 850.00  \n",
      "\n",
      "Tipos de dados:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 853 entries, 0 to 852\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tipo       853 non-null    object \n",
      " 1   bairro     853 non-null    object \n",
      " 2   cidade     853 non-null    object \n",
      " 3   objetivo   853 non-null    object \n",
      " 4   area_util  853 non-null    float64\n",
      " 5   quartos    853 non-null    int64  \n",
      " 6   suites     853 non-null    int64  \n",
      " 7   vagas      853 non-null    int64  \n",
      " 8   preco      853 non-null    float64\n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 60.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset de venda\n",
    "try:\n",
    "    df_venda = pd.read_csv('../Dados/imoveis_locacao_filtrado.csv')\n",
    "    print(\"Dados de venda carregados com sucesso!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'imoveis_locacao_filtrado.csv' não encontrado.\")\n",
    "\n",
    "print(\"\\nVisão inicial dos dados:\")\n",
    "print(df_venda.head())\n",
    "print(\"\\nTipos de dados:\")\n",
    "df_venda.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477655b7",
   "metadata": {},
   "source": [
    "## Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e0e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Engenharia de Features...\n",
      "Novas features criadas: 'tem_suite', 'tem_vaga'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIniciando Engenharia de Features...\")\n",
    "\n",
    "# 1. Criar features binárias simples\n",
    "df_venda['tem_suite'] = (df_venda['suites'] > 0).astype(int)\n",
    "df_venda['tem_vaga'] = (df_venda['vagas'] > 0).astype(int)\n",
    "\n",
    "print(\"Novas features criadas: 'tem_suite', 'tem_vaga'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556e0ce",
   "metadata": {},
   "source": [
    "## Definição de Features (X) e Alvo (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ef94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variável alvo 'y' criada com np.log1p(preco). Média (log): 8.3232\n",
      "Features (X) e Alvo (y) definidos.\n",
      "Total de features em X: 6 numéricas e 2 categóricas.\n"
     ]
    }
   ],
   "source": [
    "# --- Variável Alvo (y) ---\n",
    "\n",
    "# Aplicar a transformação logarítmica no PRECO. Isso estabiliza a variância e melhora a performance de todos os modelos.\n",
    "y = np.log1p(df_venda['preco'])\n",
    "\n",
    "print(f\"\\nVariável alvo 'y' criada com np.log1p(preco). Média (log): {y.mean():.4f}\")\n",
    "\n",
    "# --- Features (X) ---\n",
    "\n",
    "# Definir quais colunas serão usadas para prever o preço\n",
    "features_numericas = [\n",
    "    'area_util', \n",
    "    'quartos', \n",
    "    'suites', \n",
    "    'vagas',\n",
    "    # Novas features:\n",
    "    'tem_suite',\n",
    "    'tem_vaga'\n",
    "]\n",
    "\n",
    "features_categoricas = [\n",
    "    'tipo',\n",
    "    'bairro'\n",
    "]\n",
    "\n",
    "X = df_venda[features_numericas + features_categoricas]\n",
    "\n",
    "print(\"Features (X) e Alvo (y) definidos.\")\n",
    "print(f\"Total de features em X: {len(features_numericas)} numéricas e {len(features_categoricas)} categóricas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01166c59",
   "metadata": {},
   "source": [
    "## Divisão em Dados de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b12552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados divididos: 682 para treino, 171 para teste.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Dados divididos: {len(X_train)} para treino, {len(X_test)} para teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcf754",
   "metadata": {},
   "source": [
    "## Construção do Pipeline de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e92a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline de pré-processamento (ColumnTransformer) construído.\n"
     ]
    }
   ],
   "source": [
    "# 1. Pipeline para features numéricas:\n",
    "#    - StandardScaler: Coloca todas as features na mesma escala (média 0, desvio 1)\n",
    "# Isso é crucial para modelos como Regressão Linear e evita que features com valores grandes (como 'area_util') dominem as com valores pequenos (como 'quartos').\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Pipeline para features categóricas:\n",
    "#    - OneHotEncoder: Transforma 'bairro' e 'tipo' em colunas binárias (0 ou 1)\n",
    "#    - handle_unknown='ignore': Se um bairro novo aparecer nos dados de teste, ele será ignorado sem quebrar o modelo.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 3. Combinar os pipelines com o ColumnTransformer\n",
    "#    Ele aplicará o transformador correto para a coluna correta.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('num', ...): Aplica o 'numeric_transformer' na lista de colunas 'features_numericas'.\n",
    "        ('num', numeric_transformer, features_numericas),\n",
    "\n",
    "        # ('cat', ...): Aplica o 'categorical_transformer' na lista 'features_categoricas'.\n",
    "        ('cat', categorical_transformer, features_categoricas)\n",
    "    ],\n",
    "    # remainder='passthrough': Define o que fazer com colunas que NÃO foram listadas em 'features_numericas' ou 'features_categoricas' 'passthrough' significa \"mantenha-as como estão\".\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline de pré-processamento (ColumnTransformer) construído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899fff7",
   "metadata": {},
   "source": [
    "## Criação dos Pipelines de Modelo Completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17dd0ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelines completos (Pré-processamento + Modelo) criados.\n"
     ]
    }
   ],
   "source": [
    "# --- Modelo 1: Regressão Linear ---\n",
    "# Cria um \"Pipeline\" (fluxo de trabalho) para o modelo.\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    # Passo 1: Aplicar o 'preprocessor' (StandardScaler + OneHotEncoder) que definimos antes.\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Passo 2: Após pré-processar, treinar o modelo de Regressão Linear.\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# --- Modelo 2: Random Forest (com parâmetros iniciais) ---\n",
    "# Cria um segundo pipeline mas com um modelo diferente.\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    # Passo 1: Aplicar o mesmo pré-processador.\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Passo 2: Treinar o modelo Random Forest.\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=100, # Define que o modelo usará 100 árvores.\n",
    "        random_state=42,  # Garante que o resultado seja o mesmo toda vez que rodar.\n",
    "        n_jobs=-1         # Usa todos os núcleos do processador para treinar mais rápido.\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Modelo 3: XGBoost (com parâmetros iniciais) ---\n",
    "# Cria o terceiro pipeline com o XGBoost\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    # Passo 1: Aplicar o mesmo pré-processador.\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Passo 2: Treinar o modelo XGBoost.\n",
    "    ('model', XGBRegressor(\n",
    "        random_state=42, # Garante resultados reprodutíveis.\n",
    "        n_jobs=-1        # Usa todos os núcleos para acelerar.\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Cria um dicionário chamado 'models' para organizar os pipelines.\n",
    "# A chave é um nome, e o valor é o objeto do pipeline.\n",
    "models = {\n",
    "    'Regressão Linear': pipeline_lr,\n",
    "    'Random Forest': pipeline_rf,\n",
    "    'XGBoost': pipeline_xgb\n",
    "}\n",
    "\n",
    "print(\"Pipelines completos (Pré-processamento + Modelo) criados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69229842",
   "metadata": {},
   "source": [
    "## Avaliação Base (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "043cd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Validação Cruzada (Baseline) ---\n",
      "Avaliando modelos nos dados de treino (CV=5)...\n",
      "Modelo: Regressão Linear\n",
      "  MAE Médio (log): 0.2811 (std: +/- 0.0280)\n",
      "Modelo: Random Forest\n",
      "  MAE Médio (log): 0.2805 (std: +/- 0.0137)\n",
      "Modelo: XGBoost\n",
      "  MAE Médio (log): 0.2808 (std: +/- 0.0110)\n",
      "--- Validação Cruzada Concluída ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Validação Cruzada (Baseline) ---\")\n",
    "print(\"Avaliando modelos nos dados de treino (CV=5)...\")\n",
    "\n",
    "# Nota: Usamos 'neg_mean_absolute_error' (Erro Médio Absoluto Negativo) porque a função cross_val_score sempre tenta maximizar uma pontuação. Como queremos minimizar o erro (MAE), usamos a versão negativa dele. O MAE real será o valor *negativo* disso, na escala LOG.\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Esta é a função principal: ela executa a Validação Cruzada. Ela pega o 'model', divide 'X_train' e 'y_train' em 5 partes ('cv=5'). Treina em 4 partes e testa em 1, repetindo isso 5 vezes. 'scoring' define a métrica (MAE negativo). 'n_jobs=-1' usa todos os núcleos do processador para rodar mais rápido. 'scores' será uma lista com 5 pontuações (uma para cada fold).\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    # Calcula a média das 5 pontuações (ex: -0.21, -0.22, -0.20...). Invertemos o sinal (multiplicamos por -1) para ter o MAE positivo (ex: 0.21). Este é o erro médio do modelo na escala LOG.\n",
    "    mae_log_mean = -np.mean(scores)\n",
    "    mae_log_std = np.std(scores)\n",
    "    \n",
    "    print(f\"Modelo: {name}\")\n",
    "    print(f\"  MAE Médio (log): {mae_log_mean:.4f} (std: +/- {mae_log_std:.4f})\")\n",
    "\n",
    "print(\"--- Validação Cruzada Concluída ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167fb48",
   "metadata": {},
   "source": [
    "## Avaliação Final no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc81381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Avaliação Final no Conjunto de Teste ---\n",
      "Modelos base treinados.\n",
      "\n",
      "Resultados da Avaliação Final (em R$):\n",
      "          Modelo    MAE (R$)    R²\n",
      "Regressão Linear R$ 1,548.14 0.666\n",
      "   Random Forest R$ 1,662.11 0.430\n",
      "         XGBoost R$ 1,770.38 0.410\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Avaliação Final no Conjunto de Teste ---\")\n",
    "\n",
    "# Treinar os modelos base\n",
    "if 'Regressão Linear' in models:\n",
    "    models['Regressão Linear'].fit(X_train, y_train)\n",
    "if 'Random Forest' in models:\n",
    "    models['Random Forest'].fit(X_train, y_train)\n",
    "if 'XGBoost' in models:\n",
    "    models['XGBoost'].fit(X_train, y_train)\n",
    "\n",
    "print(\"Modelos base treinados.\")\n",
    "\n",
    "# Cria um dicionário vazio para guardar as previsões\n",
    "predictions = {}\n",
    "# Cria uma lista vazia para guardar as métricas (MAE, R²)\n",
    "results_list = []\n",
    "\n",
    "# Converter y_test (log) de volta para a escala original (R$)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # 1. Fazer previsão na escala LOG\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    \n",
    "    # 2. Converter previsão de volta para a escala ORIGINAL (R$)\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Salvar previsões originais\n",
    "    predictions[name] = y_pred_original\n",
    "    \n",
    "    # 3. Calcula as métricas de performance, comparando o Real vs. Previsto.\n",
    "    \n",
    "    # MAE (Erro Médio Absoluto): A métrica principal. Diz, em média, \"quantos Reais\" o modelo errou (para mais ou para menos).\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "\n",
    "    # R² (R-quadrado): A métrica de \"qualidade do ajuste\". Diz qual porcentagem da variação dos preços o modelo conseguiu explicar (0 a 1).\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    \n",
    "    # Adiciona os resultados deste modelo na lista.\n",
    "    results_list.append({'Modelo': name, 'MAE (R$)': mae, 'R²': r2})\n",
    "\n",
    "# Transforma a lista de resultados em uma tabela (DataFrame) do Pandas.\n",
    "# Ordena a tabela pelo 'MAE (R$)' (do menor para o maior) para que o melhor modelo apareça primeiro.\n",
    "df_results = pd.DataFrame(results_list).sort_values(by='MAE (R$)')\n",
    "\n",
    "print(\"\\nResultados da Avaliação Final (em R$):\")\n",
    "\n",
    "# Imprime a tabela de resultados de forma legível:\n",
    "# .to_string() -> Converte o DataFrame para texto.\n",
    "# formatters={...} -> Define a formatação (MAE como \"R$ 123.456,78\" e R² como \"0.745\").\n",
    "# index=False -> Esconde os números de índice (0, 1, 2...) do DataFrame.\n",
    "print(df_results.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc0ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a4546",
   "metadata": {},
   "source": [
    "## Otimização da Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edcda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Otimização da Regressão Linear ---\n",
      "Novo pré-processador com PolynomialFeatures criado.\n",
      "Novos modelos (Ridge e Lasso) criados.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Otimização da Regressão Linear ---\")\n",
    "\n",
    "# 1. CRIAR UM NOVO PRÉ-PROCESSADOR COM FEATURES POLINOMIAIS\n",
    "\n",
    "# O objetivo é capturar relações não-lineares (ex: area², area * quartos)\n",
    "\n",
    "# 1a. Novo Pipeline Numérico (com PolynomialFeatures + Scaler)\n",
    "numeric_transformer_poly = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 1b. Pipeline Categórico (igual ao anterior)\n",
    "categorical_transformer_poly = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 1c. Novo ColumnTransformer (usa os pipelines acima)\n",
    "preprocessor_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_poly, features_numericas),\n",
    "        ('cat', categorical_transformer_poly, features_categoricas)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Novo pré-processador com PolynomialFeatures criado.\")\n",
    "\n",
    "# 2. DEFINIR OS NOVOS MODELOS LINEARES OTIMIZADOS\n",
    "\n",
    "# Lista de 'alphas' (força da penalidade) para os modelos testarem\n",
    "alphas_para_testar = [0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 50.0]\n",
    "\n",
    "# 2a. Modelo Ridge (L2): Encolhe os coeficientes\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_poly),\n",
    "    ('model', RidgeCV(alphas=alphas_para_testar))\n",
    "])\n",
    "\n",
    "# 2b. Modelo Lasso (L1): Pode zerar coeficientes inúteis\n",
    "pipeline_lasso = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_poly),\n",
    "    ('model', LassoCV(alphas=alphas_para_testar, cv=5, max_iter=5000)) # Lasso pode precisar de mais iterações\n",
    "])\n",
    "\n",
    "# 2c. Dicionário com os novos modelos\n",
    "models_otimizados = {\n",
    "    'Regressão Linear (Otimizada com Ridge)': pipeline_ridge,\n",
    "    'Regressão Linear (Otimizada com Lasso)': pipeline_lasso\n",
    "}\n",
    "\n",
    "print(\"Novos modelos (Ridge e Lasso) criados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3227163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Avaliação dos Modelos Otimizados ---\n",
      "Treinando e avaliando: Regressão Linear (Otimizada com Ridge)...\n",
      "Treinando e avaliando: Regressão Linear (Otimizada com Lasso)...\n",
      "\n",
      "--- Resultados da Avaliação OTIMIZADA (em R$) ---\n",
      "                                Modelo    MAE (R$)    R²\n",
      "Regressão Linear (Otimizada com Ridge) R$ 1,523.06 0.596\n",
      "Regressão Linear (Otimizada com Lasso) R$ 2,015.27 0.307\n",
      "\n",
      "--- Para Comparação (Resultados da Célula 11) ---\n",
      "          Modelo    MAE (R$)    R²\n",
      "Regressão Linear R$ 1,548.14 0.666\n",
      "   Random Forest R$ 1,662.11 0.430\n",
      "         XGBoost R$ 1,770.38 0.410\n"
     ]
    }
   ],
   "source": [
    "# TREINAR E AVALIAR OS NOVOS MODELOS\n",
    "\n",
    "print(\"\\n--- Iniciando Avaliação dos Modelos Otimizados ---\")\n",
    "\n",
    "results_list_otimizada = []\n",
    "\n",
    "for name, model in models_otimizados.items():\n",
    "    \n",
    "    print(f\"Treinando e avaliando: {name}...\")\n",
    "    \n",
    "    # Treina o modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Faz a previsão (em log)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    \n",
    "    # Converte de volta para R$\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Salva as previsões\n",
    "    predictions[name] = y_pred_original\n",
    "    \n",
    "    # Calcula as métricas\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    \n",
    "    results_list_otimizada.append({'Modelo': name, 'MAE (R$)': mae, 'R²': r2})\n",
    "\n",
    "\n",
    "# 4. EXIBIR OS RESULTADOS FINAIS DA OTIMIZAÇÃO\n",
    "\n",
    "df_results_otimizada = pd.DataFrame(results_list_otimizada).sort_values(by='MAE (R$)')\n",
    "\n",
    "print(\"\\n--- Resultados da Avaliação OTIMIZADA (em R$) ---\")\n",
    "print(df_results_otimizada.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))\n",
    "\n",
    "print(\"\\n--- Para Comparação (Resultados da Célula 11) ---\")\n",
    "print(df_results.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b77172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Otimização V2 (Polynomial Seletivo) ---\n",
      "Novo pré-processador (V2) seletivo criado.\n",
      "Novos modelos (Ridge V2) criados.\n",
      "\n",
      "--- Iniciando Avaliação do Modelo Otimizado V2 ---\n",
      "Treinando e avaliando: Regressão Linear (Otimizada V2 - Ridge)...\n",
      "\n",
      "--- Resultados da Avaliação OTIMIZADA (V2) ---\n",
      "                                 Modelo    MAE (R$)    R²\n",
      "Regressão Linear (Otimizada V2 - Ridge) R$ 1,473.18 0.596\n",
      "\n",
      "--- Para Comparação (Resultados Anteriores) ---\n",
      "Modelo Otimizado (V1):\n",
      "                                Modelo    MAE (R$)    R²\n",
      "Regressão Linear (Otimizada com Ridge) R$ 1,523.06 0.596\n",
      "Regressão Linear (Otimizada com Lasso) R$ 2,015.27 0.307\n",
      "\n",
      "Modelo Original (Célula 11):\n",
      "          Modelo    MAE (R$)    R²\n",
      "Regressão Linear R$ 1,548.14 0.666\n",
      "   Random Forest R$ 1,662.11 0.430\n",
      "         XGBoost R$ 1,770.38 0.410\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Iniciando Otimização V2 (Polynomial Seletivo) ---\")\n",
    "\n",
    "# 1. DEFINIR AS FEATURES NUMÉRICAS\n",
    "\n",
    "# Feature que receberá PolynomialFeatures (relação não-linear)\n",
    "features_poly = ['area_util']\n",
    "\n",
    "# Features que NÃO receberão PolynomialFeatures (relação linear)\n",
    "features_linear = [\n",
    "    'quartos', \n",
    "    'suites', \n",
    "    'vagas',\n",
    "    'tem_suite',\n",
    "    'tem_vaga',\n",
    "    'area_por_quarto', \n",
    "    'proporcao_suites'\n",
    "]\n",
    "\n",
    "# Garante que as listas estão corretas e não contêm features que não existem\n",
    "features_poly = [f for f in features_poly if f in X_train.columns]\n",
    "features_linear = [f for f in features_linear if f in X_train.columns]\n",
    "\n",
    "\n",
    "# 2. CRIAR UM PRÉ-PROCESSADOR MAIS INTELIGENTE\n",
    "\n",
    "# 2a. Pipeline para 'area_util' (Polynomial + Scaler)\n",
    "numeric_transformer_poly = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2b. Pipeline para as outras features numéricas (Apenas Scaler)\n",
    "numeric_transformer_linear = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2c. Pipeline Categórico (igual ao anterior)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 2d. Novo ColumnTransformer (que junta os 3 pipelines)\n",
    "preprocessor_v2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poly', numeric_transformer_poly, features_poly), # Pipeline 1\n",
    "        ('linear', numeric_transformer_linear, features_linear), # Pipeline 2\n",
    "        ('cat', categorical_transformer, features_categoricas) # Pipeline 3\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Novo pré-processador (V2) seletivo criado.\")\n",
    "\n",
    "# 3. DEFINIR O MODELO RIDGE COM ALPHAS REFINADOS\n",
    "\n",
    "# Alphas mais \"finos\" e focados em valores menores\n",
    "alphas_refinados = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "pipeline_ridge_v2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_v2), # Usa o novo pré-processador V2\n",
    "    ('model', RidgeCV(alphas=alphas_refinados))\n",
    "])\n",
    "\n",
    "models_otimizados_v2 = {\n",
    "    'Regressão Linear (Otimizada V2 - Ridge)': pipeline_ridge_v2\n",
    "}\n",
    "\n",
    "print(\"Novos modelos (Ridge V2) criados.\")\n",
    "\n",
    "# 4. TREINAR E AVALIAR O NOVO MODELO\n",
    "\n",
    "print(\"\\n--- Iniciando Avaliação do Modelo Otimizado V2 ---\")\n",
    "\n",
    "results_list_v2 = []\n",
    "\n",
    "for name, model in models_otimizados_v2.items():\n",
    "    \n",
    "    print(f\"Treinando e avaliando: {name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "    \n",
    "    predictions[name] = y_pred_original\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    \n",
    "    results_list_v2.append({'Modelo': name, 'MAE (R$)': mae, 'R²': r2})\n",
    "\n",
    "# 5. EXIBIR OS RESULTADOS FINAIS (V2)\n",
    "\n",
    "df_results_v2 = pd.DataFrame(results_list_v2).sort_values(by='MAE (R$)')\n",
    "\n",
    "print(\"\\n--- Resultados da Avaliação OTIMIZADA (V2) ---\")\n",
    "print(df_results_v2.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))\n",
    "\n",
    "print(\"\\n--- Para Comparação (Resultados Anteriores) ---\")\n",
    "print(\"Modelo Otimizado (V1):\")\n",
    "print(df_results_otimizada.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))\n",
    "print(\"\\nModelo Original (Célula 11):\")\n",
    "print(df_results.to_string(formatters={'MAE (R$)': 'R$ {:,.2f}'.format, 'R²': '{:.3f}'.format}, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a96d35",
   "metadata": {},
   "source": [
    "## Análise de Coeficientes do Modelo Otimizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aeed0c",
   "metadata": {},
   "source": [
    "### Modelo V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c089d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análise de Coeficientes (Regressão Linear Otimizada) ---\n",
      "Analisando o modelo: Regressão Linear (Otimizada com Ridge)\n",
      "\n",
      "Alpha (penalidade) escolhido pelo modelo: 0.5\n",
      "\n",
      "--- Top 15 Features que MAIS AUMENTAM o aluguel ---\n",
      "                        Feature Coeficiente (em log)\n",
      "53          bairro_higienopolis               0.6701\n",
      "130  bairro_vila_nova_conceicao               0.6650\n",
      "67         bairro_jardim_europa               0.6599\n",
      "0                     area_util               0.6517\n",
      "36         bairro_brooklin_novo               0.6290\n",
      "35              bairro_brooklin               0.6042\n",
      "40       bairro_cerqueira_cesar               0.5980\n",
      "56            bairro_itaim_bibi               0.5956\n",
      "47        bairro_cidade_moncoes               0.5947\n",
      "93           bairro_real_parque               0.5598\n",
      "131         bairro_vila_olimpia               0.5447\n",
      "73     bairro_jardim_paulistano               0.5376\n",
      "91             bairro_pinheiros               0.4822\n",
      "72       bairro_jardim_paulista               0.4422\n",
      "41      bairro_chacara_gaivotas               0.4373\n",
      "\n",
      "--- Top 15 Features que MAIS DIMINUEM o aluguel ---\n",
      "                           Feature Coeficiente (em log)\n",
      "107   bairro_vila_brasilio_machado              -0.3971\n",
      "109          bairro_vila_campestre              -0.4064\n",
      "51                  bairro_cursino              -0.4278\n",
      "76       bairro_jardim_previdencia              -0.4287\n",
      "129             bairro_vila_moraes              -0.4381\n",
      "75     bairro_jardim_portal_i_e_ii              -0.4726\n",
      "95                   bairro_sacoma              -0.4766\n",
      "60           bairro_jardim_celeste              -0.4990\n",
      "118            bairro_vila_fachini              -0.5181\n",
      "69   bairro_jardim_miriam_zona_sul              -0.5240\n",
      "117        bairro_vila_do_encontro              -0.5337\n",
      "32           bairro_americanopolis              -0.5505\n",
      "111              bairro_vila_clara              -0.5662\n",
      "84                  bairro_morumbi              -0.5816\n",
      "103         bairro_vila_agua_funda              -0.8969\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Análise de Coeficientes (Regressão Linear Otimizada) ---\")\n",
    "\n",
    "# Escolha o nome do seu melhor modelo da célula anterior\n",
    "nome_do_melhor_modelo = df_results_otimizada.iloc[0]['Modelo'] \n",
    "\n",
    "print(f\"Analisando o modelo: {nome_do_melhor_modelo}\")\n",
    "\n",
    "try:\n",
    "    # 1. Pega o pipeline treinado\n",
    "    pipeline_treinado = models_otimizados[nome_do_melhor_modelo]\n",
    "    \n",
    "    # 2. Pega o modelo (RidgeCV ou LassoCV) de dentro do pipeline\n",
    "    modelo_final = pipeline_treinado.named_steps['model']\n",
    "\n",
    "    # 3. Pega o pré-processador de dentro do pipeline\n",
    "    preprocessor_fitted = pipeline_treinado.named_steps['preprocessor']\n",
    "\n",
    "    # --- Pegar todos os nomes das features (isto é complexo) ---\n",
    "    # 3a. Nomes das features numéricas (criadas pelo PolynomialFeatures)\n",
    "    num_features_poly = (\n",
    "        preprocessor_fitted.named_transformers_['num'] \n",
    "        .named_steps['poly']\n",
    "        .get_feature_names_out(features_numericas)\n",
    "    )\n",
    "    \n",
    "    # 3b. Nomes das features categóricas (criadas pelo OneHotEncoder)\n",
    "    cat_features_out = (\n",
    "        preprocessor_fitted.named_transformers_['cat']\n",
    "        .named_steps['onehot']\n",
    "        .get_feature_names_out(features_categoricas)\n",
    "    )\n",
    "    \n",
    "    # 3c. Lista final com TODOS os nomes\n",
    "    all_features_out = list(num_features_poly) + list(cat_features_out)\n",
    "\n",
    "    # 4. Pega os coeficientes (os \"pesos\" que o modelo deu a cada feature)\n",
    "    coefs = modelo_final.coef_\n",
    "\n",
    "    # 5. Cria o DataFrame de coeficientes\n",
    "    df_coefs = pd.DataFrame({\n",
    "        'Feature': all_features_out,\n",
    "        'Coeficiente (em log)': coefs\n",
    "    }).sort_values(by='Coeficiente (em log)', ascending=False)\n",
    "    \n",
    "    # 6. Exibe os resultados\n",
    "    print(\"\\nAlpha (penalidade) escolhido pelo modelo:\", modelo_final.alpha_)\n",
    "\n",
    "    print(\"\\n--- Top 15 Features que MAIS AUMENTAM o aluguel ---\")\n",
    "    df_coefs_head = df_coefs[df_coefs['Feature'].str.len() < 40].head(15)\n",
    "    print(df_coefs_head.to_string(formatters={'Coeficiente (em log)': '{:,.4f}'.format}))\n",
    "\n",
    "    print(\"\\n--- Top 15 Features que MAIS DIMINUEM o aluguel ---\")\n",
    "    df_coefs_tail = df_coefs[df_coefs['Feature'].str.len() < 40].tail(15)\n",
    "    print(df_coefs_tail.to_string(formatters={'Coeficiente (em log)': '{:,.4f}'.format}))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Não foi possível extrair coeficientes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b035e2",
   "metadata": {},
   "source": [
    "### Modelo V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476ef849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análise de Coeficientes (Regressão Linear Otimizada V2) ---\n",
      "Analisando o modelo: Regressão Linear (Otimizada V2 - Ridge)\n",
      "\n",
      "Alpha (penalidade) escolhido pelo modelo: 1.0\n",
      "\n",
      "--- Top 15 Features que MAIS AUMENTAM o aluguel ---\n",
      "                         Feature Coeficiente (em log)\n",
      "110   bairro_vila_nova_conceicao               0.6378\n",
      "0                      area_util               0.6266\n",
      "65   bairro_paineiras_do_morumbi               0.5933\n",
      "36             bairro_itaim_bibi               0.5879\n",
      "27         bairro_cidade_moncoes               0.5423\n",
      "47          bairro_jardim_europa               0.5382\n",
      "20        bairro_cerqueira_cesar               0.5079\n",
      "15               bairro_brooklin               0.5028\n",
      "53      bairro_jardim_paulistano               0.4953\n",
      "16          bairro_brooklin_novo               0.4829\n",
      "33           bairro_higienopolis               0.4760\n",
      "111          bairro_vila_olimpia               0.4578\n",
      "73            bairro_real_parque               0.4550\n",
      "17      bairro_brooklin_paulista               0.3809\n",
      "30             bairro_consolacao               0.3754\n",
      "\n",
      "--- Top 15 Features que MAIS DIMINUEM o aluguel ---\n",
      "                          Feature Coeficiente (em log)\n",
      "43         bairro_jardim_da_saude              -0.3383\n",
      "28           bairro_cidade_vargas              -0.3427\n",
      "56      bairro_jardim_previdencia              -0.3742\n",
      "51         bairro_jardim_oriental              -0.3759\n",
      "64                 bairro_morumbi              -0.3843\n",
      "55    bairro_jardim_portal_i_e_ii              -0.3925\n",
      "89          bairro_vila_campestre              -0.3978\n",
      "83         bairro_vila_agua_funda              -0.4161\n",
      "40          bairro_jardim_celeste              -0.4244\n",
      "91              bairro_vila_clara              -0.4533\n",
      "75                  bairro_sacoma              -0.4679\n",
      "98            bairro_vila_fachini              -0.5151\n",
      "12          bairro_americanopolis              -0.5404\n",
      "49  bairro_jardim_miriam_zona_sul              -0.5517\n",
      "97        bairro_vila_do_encontro              -0.5717\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Análise de Coeficientes (Regressão Linear Otimizada V2) ---\")\n",
    "\n",
    "# Vamos usar os resultados do DataFrame V2\n",
    "nome_do_melhor_modelo = df_results_v2.iloc[0]['Modelo'] \n",
    "\n",
    "print(f\"Analisando o modelo: {nome_do_melhor_modelo}\")\n",
    "\n",
    "try:\n",
    "    # 1. Pega o pipeline treinado (do dicionário V2)\n",
    "    pipeline_treinado = models_otimizados_v2[nome_do_melhor_modelo]\n",
    "    \n",
    "    # 2. Pega o modelo (RidgeCV) de dentro do pipeline\n",
    "    modelo_final = pipeline_treinado.named_steps['model']\n",
    "\n",
    "    # 3. Pega o pré-processador (V2) de dentro do pipeline\n",
    "    preprocessor_fitted = pipeline_treinado.named_steps['preprocessor']\n",
    "\n",
    "    # --- Pegar todos os nomes das features ---\n",
    "    \n",
    "    # 3a. Nomes das features POLINOMIAIS (do pipeline 'poly')\n",
    "    poly_feature_names = (\n",
    "        preprocessor_fitted.named_transformers_['poly'] # Pega o pipeline 'poly'\n",
    "        .named_steps['poly'] # Pega o passo 'poly'\n",
    "        .get_feature_names_out(features_poly) # Pega os nomes (usando 'features_poly')\n",
    "    )\n",
    "    \n",
    "    # 3b. Nomes das features NUMÉRICAS LINEARES (do pipeline 'linear')\n",
    "    #    (O StandardScaler não muda os nomes, então usamos a lista original)\n",
    "    linear_feature_names = features_linear\n",
    "    \n",
    "    # 3c. Nomes das features CATEGÓRICAS (do pipeline 'cat')\n",
    "    cat_features_out = (\n",
    "        preprocessor_fitted.named_transformers_['cat'] # Pega o pipeline 'cat'\n",
    "        .named_steps['onehot'] # Pega o passo 'onehot'\n",
    "        .get_feature_names_out(features_categoricas) # Pega os nomes\n",
    "    )\n",
    "    \n",
    "    # 3d. Lista final com TODOS os nomes (na ordem correta: poly, linear, cat)\n",
    "    all_features_out = list(poly_feature_names) + list(linear_feature_names) + list(cat_features_out)\n",
    "\n",
    "    # 4. Pega os coeficientes \n",
    "    coefs = modelo_final.coef_\n",
    "\n",
    "    # 5. Cria o DataFrame de coeficientes\n",
    "    df_coefs = pd.DataFrame({\n",
    "        'Feature': all_features_out,\n",
    "        'Coeficiente (em log)': coefs\n",
    "    }).sort_values(by='Coeficiente (em log)', ascending=False)\n",
    "    \n",
    "    # 6. Exibe os resultados\n",
    "    print(\"\\nAlpha (penalidade) escolhido pelo modelo:\", modelo_final.alpha_)\n",
    "\n",
    "    print(\"\\n--- Top 15 Features que MAIS AUMENTAM o aluguel ---\")\n",
    "    df_coefs_head = df_coefs[df_coefs['Feature'].str.len() < 40].head(15)\n",
    "    print(df_coefs_head.to_string(formatters={'Coeficiente (em log)': '{:,.4f}'.format}))\n",
    "\n",
    "    print(\"\\n--- Top 15 Features que MAIS DIMINUEM o aluguel ---\")\n",
    "    df_coefs_tail = df_coefs[df_coefs['Feature'].str.len() < 40].tail(15)\n",
    "    print(df_coefs_tail.to_string(formatters={'Coeficiente (em log)': '{:,.4f}'.format}))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Não foi possível extrair coeficientes: {e}\")\n",
    "    print(\"Verifique se as variáveis 'models_otimizados_v2', 'df_results_v2', 'features_poly', e 'features_linear' existem.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
